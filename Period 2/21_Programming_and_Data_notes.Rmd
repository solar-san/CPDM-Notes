---
title: |
  | Computer Programming and Data Management 2
author: |
  | solar-san
date: |
  | II Period 2022/2023
output:
  html_document: 
    css: custom.css
    theme: united
    toc: yes
    toc_float: yes
    number_section: yes
    fig_width: 8
  pdf_document:
    highlight: zenburn
    toc: yes
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
header-includes:
  - \usepackage{xcolor}
editor_options: 
  markdown: 
    wrap: sentence
---

```{r notebook setup, include = FALSE}
setwd("~/Documents/DABS/Appunti/Computer_Programming_and_Data_Management/Notes/Period 2/CP_Notes_RMarkdown")
```

```{python python setup, include = FALSE}
import gc
import my_first_module as cs
import timeit

def type_print(obj):
  print(obj, '--->', type(obj))
```




# Introduction

The main topic of the course is how to *treat* data, from different sources and from different formats: *load, inspect, wrangle, handle*. Also, we will see some *automatic treatment of data*, such as **machine learning** techniques.

The main topics are focused on **data analytics**:

> * The Python programming language.
> * Data-oriented library ecosystem and tools.

Essential libraries are:

* `NumPy` (*Numerical Python*): cornerstone library for scientific computing in `Python.` It provides the data structures, algorithms, and library glue needed for most scientific applications involving ***numerical data***.
  * Beyond the fast array-processing capabilities that `NumPy` adds to `Python`, one of its primary uses in data analysis is as a **container for data to be passed between algorithms and libraries**.
* `pandas`: provides high-level data structures and functions designed to work with ***structured*** or ***tabular*** data intuitively and flexibly.
  * It provides convenient **indexing functionality** to enable you to reshape, slice and dice, perform aggregations, and select subsets of data. 
* `matplotlib`: the most popular library for producing **plots** and ***bi-dimensional data visualisations***.
* `SciPy`: a collection of packages addressing a number of foundational problems in ***scientific computing***.
* `scikit-learn`: general purpose ***machine learning*** toolkit.
* `statsmodels`: ***statistical analysis package***.
  * `statsmodels` is more focused on *statistical inference*, providing uncertainty estimates and p-values for parameters. `scikit-learn`, by contrast, is more prediction focused.

---




# Built-in Data Structures, Functions and Files

Before moving onto *specialised data analytics* packages, re-discovering `python`'s **built-in data structure** is fundamental, because most of the latter are designed to work in tandem with the former, *vanilla* structure of the programming language.

```{python importing modules}
import shutil
import urllib.request
```

Libraries are collection of functions, structured to yield specific pre-compiled Python code. They work as **programming environment**, with a peculiar collection of objects and functions readily available to perform specialized tasks. They are loaded with the `import` statement; the second one, `urllib` (or any other specific name, such as `pandas` or `Numpy` or whatever), contains functions to download and upload from the web. The first one contains functions used to *package* or *unpackage* `.zip` files.

> **NB**: most of the following commands only work for `colab` and other settings, not for **base** `python`. neither for `RMarkdown` or `Visual Studio Code`.

`ls` lists the file on your computer: can be used to print a list of all the files present in the current working directory.

```{python checking ls, eval = FALSE}
!ls
```

This is mostly useful to check the content of your working directory. The **syntax** of this kind of command is mostly the same as the one used in **LINUX/UNIX** terminals scripting.

By adding a `?` at the end of a *name* (such as a variable of a function call) the prompt shows the relative **help documentation**.

```{python help docs, eval = FALSE}
urllib?
```

By adding `?` after a named function the relative code is printed on screen: it is useful to investigate how a program is actually computing a specific task. 

A syntax such as `urllib.*?` prints instead all the available methods. A number of characters combined with the wildcard (\*) will show all names matching the wildcard expression. `??` access **web help pages** if available.

> A very important characteristic of `python` is the **consistency of its *object model* **. Every *number, string, data structure, function, class, module,* and so on exists in the Python interpreter in its own **box**, which is referred to as a **Python object**. Each object has an associated ***type*** (e.g., integer, string, or function) and internal data.

To tidy up your space, `del` and **garbage colletion** are your best friend:

```{python deleting objects from workspace, eval = F}
del not_useful_stuff
```

This simply unbinds a name from an object; then, invoke the `gc.collect()` method.

---

Python has a very powerful built-in *native data structure*, offering high level functionalities.


## `tuple`

> A **tuple** is an **unmutable ordered sequence of objects**; any iterable in Python can be converted in a `tuple` with its keyword. Parenthesis are *optional* but are recommended when performing complex assignments.

```{python tuples creation}
tpl = ((4,5,6), ('a','b'))
print(tpl)
```

Subsetting is done with `[]`; however elements are ***read only***, thus it cannot be modified.

```{python subsetting tuples}
tpl[1][1]
```

Trying to modify a *tuple* prompts a `TypeError` message:

```{python tuples are unmutable, error = TRUE}
tpl[1][1] = 'c'
```

If a tuple's element is *mutable*, **mutators** methods can be invoked to perform operations on it.

```{python adding elements to tuples 1}
tpl = (True, [1,2,3])
tpl[1].append('I just modified a tuple..or did I? O.o')
print(tpl)
```

>What actually is not mutable is the **order** in which objects are stored and the objects themselves; nevertheless, if such objects are of a *mutable type*, then you can work with them as you normally would. In other words, you can modify them in place, but not remove them or switch their position inside the sequence.

Operators might be used to produce *new tuples* by concatenating existing or new tuples:

```{python adding elements to tuples 2}
type_print(tpl + ('foo',))
```

```{python tuples multiplication}
tpl1 = 1, 2, 3

tpl1 * 3
```


### Unpacking Tuples

Tuples can be easily unpacked by using **tuples assignment**:

```{python tuples assignment 1}
tup = (4,5,6)
a, b, c = tup
print(a, b, c)
```

You can perform more complex assignments:

```{python tuples assignment 2}
tup = 4, 5, (6, 7)
a, b, (c, d) = tup
print(tup)
```

```{python tuples assignment 3}
print(d)
```

This comes really handy to solve quickly the old **variable swap** problem:

```{python variable swap with tuples 1}
a, b, = 1, 2
print(a, b)
```

```{python variable swap with tuples 2}
b, a = a, b
print(a, b)
```

Also, it is a very handy way to iterate over composite types: each iteration in the following code block assigns `a, b, c` to each value of the nested tuples.

```{python iteration with tuples}
seq = [(1,2,3),(4,5,6),(7,8,9)]
for a, b, c in seq:
  print('a = {0}, b = {1}, c = {2}'.format(a, b, c))
```

The `gather` and `scatter` operator `*` is useful to avoid capturing all the items.

```{python gather 1}
values = 1, 2, 3, 4, 5
a, b, *rest = values
print(rest)
```

If you are not interested in assigning a variable, you can use the conventional name `_`, which stands for *Yeah, I don't really care about that stuff*:

```{python gather 2}
a, b, *_ = values
print(_)
```


## `list`

A **list** is an **ordered, mutable sequence of objects**. It is nothing more than a *mutable tuple*, if you will; as for the tuple, it has a constructor `list()` which accepts any *iterable* object to build a list with its elements.

```{python lists}
print(list(values), '--->', type(list(values)))
```

Lists can also be initialized by using **generators**, such as `range`; this kind of functions creates specific objects (in this case, the `range object`) which are **iterable** objects that can create lists.

```{python generators and lists}
gen = range(10)
type_print(gen)
```

A generator **does not store all its elements in memory**, yielding a powerful tool to avoid wasting **RAM** and slowing down the program.

```{python generators and memory}
print(list(gen))
```

> Basically, *lists and tuples* are a useful way to **materialize generator objects**, which are a safe-way to store and generate data in a memory-safe way.


### Hashable, or Not Hashable?

> **Hash maps** or **associative arrays** are used to perform searches in a fast and memory-safe way.

For an object to be **hashable, *all its elements* must be hashable**.

```{python hashable/not hashable 1}
hashable = 1, 2
hash(hashable)
```

```{python hashable/not hashable 2, error = TRUE}
not_hashable = 1, 2, [3, 4]
hash(not_hashable)
```

Set elements must necessarily be **hashable**: in way **sets** are *dictionaries without keys*. 


## Binary Search

A **binary search** algorithm can be imported with the `bisect` module. `bisect.bisect` finds the location where an element should be inserted to keep it sorted, while `bisect.insort` actually inserts the element into that location.

```{python bisection}
import bisect
import time

huge_list = range(0, int(1e10))

start = time.time()
bisect.bisect(huge_list, 1516782)
stop = time.time()

print(stop - start, 'seconds')
```

```{python no bisection}
start1 = time.time()
huge_list.index(1516782)
stop1 = time.time()
print(stop1 - start1, 'seconds')
```


## Built-in Sequence Functions

These built-in functions will work on **any iterable**; some of them create ***generators objects**, which are a very useful format to store large sequences in a memory-friendly way.


### `enumerate`

`enumerate` keeps track of the **index** over a collection of elements: in other words, this

```{python enumerate with loop 1, eval = F}
index = 0
for value in collection:
   # do something with value
   index += 1
```

can be done with this:

```{python enumerate, eval = F}
for index, value in enumerate(collection):
   # do something with value
```


### `sorted`

`sorted` returns a **sorted** list of any sequence.


### `zip`

`zip` “pairs” up the elements of a number of lists, tuples, or other sequences to create a list of tuples and can take an arbitrary number of sequences, and the number of elements it produces is determined by the shortest sequence.


### `reversed`

`reversed` iterates over the elements of a sequence in reverse order.


## `List`, `Set` and `Dict` Comprehensions

**List comprehension** yields a powerful method to generate sequence of objects and filter them. They take the basic form:

```{python comprehension sintax, eval = FALSE}
[expr for value in collection if condition]
```

This method can be used to generate different objects; not only *lists*, but also *sets*, *dictionaries*, *tuples*:

```{python set comprehension}
x = {x for x in range(0, 4)}
type_print(x)
```

```{python dict comprehension}
y = {y: y**2 for y in range(0,11)}
type_print(y)
```

**Nested list comprehension** is also an option: the `for` parts of the list comprehension are arranged according to the order of nesting, and any filter condition is put at the end as before.

```{python nested list comprehension 1}
nested_names = [['John', 'Emily', 'Michael', 'Mary', 'Steven'], ['Maria', 'Juan', 'Javier', 'Natalia', 'Pilar']]
print(nested_names)
```

```{python nested list comprehension 2}
flattened_names = [name for names in nested_names for name in names]
print(flattened_names)
```




# Functions


## Namespaces, Scope and Local Functions

> ***Namespace***: variable scope.

Within functions it is possible to define **local variables**. They are said to belong to the **local namespace**, or **local scope**.
The local scope only lives within the function (with some exceptions). **Global** variables are said to be *shadowed* by local variables inside the local scope, even though they cannot be *permanently* modified inside the function. After a function is done running, its namespace is **destroyed** and with it all its local variables.

Usually:

> **Global**: defines the state of the system. Better leave that alone. If you use `global` a lot, try creating an *object* and invoke *methods* instead.


## Functions Are Objects

It is important to note that **functions *also* are objects**: this means that *a function can be treated like a value and applied.* The fundamental concept of **functional programming** is exactly this: write functions like you would create building blocks and them assembly them to perform complex operations.

For example, `map` allows you to build an iterable `map object` containing the results of the application of a function to all the elements of a iterable. It can be used as an alternative to *list comprehension*, without filters.

```{python map function 1}
numbers = [1, -2, 4, -3, -5, 6]
mapping = map(abs, numbers)
type_print(mapping)
```

To convert this object into a sequence, you can use whatever function you need:

```{python map function 2}
list(mapping)
```


## Lambda Functions

**Lambda** ($\lambda$) is a simple *keyword* to create *anonymous functions*. They are mostly used for coding *short* functions, for which a single line of code is sufficient: they consist in **single statements**, the result of which prints a return value.\ 
For example:

```{python lambda function 1}
lambda_exp = lambda x, y: x**y

lambda_exp(x = 2, y = 3)
```

It can be very useful whenever a function expects another one as a parameter.




# Errors and Exception Handling

Errors are common, especially with *user generated input*. `try` and `except` statements are a powerful tool to **handle errors**. Each `except` statement can be coded to handle a specific `error`:

```{python exception handling 1}
def attempt_float(x):
    try:
      return float(x)
    except TypeError:
      print("Hey guy, you didn't event get the type right !")
      return type_print(x)
    except ValueError:
      print("Almost, but no cigar!")
      return x
attempt_float([1,2])

```

```{python exception handling 2}
attempt_float('1.2.3')
```

> Beware that in this case **if no `except` statement captures the kind of error happening** it will reach the operating system and **halt the execution of the program**.

The `finally` statement use case is for whenever you have a code block that you want to run **regardless of any exception happening or not**; the `else` statement introduces a code block which runs only if the `try` statement succeeded:

```{python exception handling with finally}
def print_if(my_string):
  try:
    print(int(my_string))
  except:
    print('Gimme a string, plz')
  finally:
    print('VERY IMPORTANT MESSAGE')

print_if('hello there')

```




# Generators

**Generators** statement are a quick and efficient way to generate iterables and sequences: they can be as complicated as you want without taking space in memory. In fact, they initialize **generator *objects* **, which can then be used as sequences or iterables.

```{python generators range}
range(int(1e10))
```

Even as it represent a sequence, **it does not actually create them**, thus being fast and efficient.
The `yield` keyword is used within a function and allows you to create a *custom generator*: it indicates where a value is sent back to the caller, but unlike return, you don’t exit the function afterwards and generate values sequentially when called. It will then *convert an expression that is specified along with it to a generator object* and * **return** it to the caller*. Hence, if you want to get the values stored inside the generator object, you need to iterate over it.

Remember: it iterates **sequentially**. This means that it will keep track of where in the sequence happened its last call. When they reach the end of their sequence, they will not *yield* other objects.

```{python generate your own generator}
def squares(n = 10):
  print('Generating squares from 1 to {0}'.format(n ** 2))
  for i in range(1, n +1):
    yield i ** 2
    
gen = squares(20)
type_print(gen)
```

Instead of using square brackets, **list comprehension** syntax with `()` can be used to initialize generator objects.

```{python comprehension syntax as a way to create generators}
gen = (x ** 2 for x in range(100) if x % 2 == 0)
type_print(gen)
```

The `iter` statements tries to create an iterable from a generator:

```{python initialize dictionary with enumerators}
power_range = {key:values for key, values in enumerate(gen)}
type_print(power_range)
```




# Modules and Import

In Python a **module** is simply a *file* with the `.py` extension. It is a way to organise objects, functions and libraries inside the program; they can contain a lot of different files and directories as a quick way to easily maintain a program, in a *modular* way, by storing what you might need to access it only if and when you need it inside a **package** known as ***namespaces***.

By using a *text editor, Colab, Jupyter notebooks* and many other ways you can save your code in a ***module*** and use the `import` statement to access the items stored inside it with the `.` invocation, which permits the access to any ***name*** define inside them module. To avoid the dot notation, you can use this syntax:

```{python import statement syntax, eval = FALSE, message = FALSE, error = FALSE}
from my_module import function, object, whatever as another name
```

The `as` statement will allow you to change **names**, which is useful to avoid masking

```{python custom module import}
import my_first_module
# ?my_first_module access all the code of the imported module.
my_first_module.type_print(3)
```

> Be always careful with ***masking*** or ***aliasing*** issues whenever working with **import** statemets and the like.

Modules can also be organize in **subdirectories**: they are accessed by using the `*.*` notation.\
You might want to store your modules this way whenever your program requires several modules and you must keep your directories tidy.




# Files and the Operating System

Recommendation:

> * Close the file when you are finished with it. Closing the file releases its resources back to the operating system.
> * Specify the encoding.

```{python reading from file}

path = '/Users/themagician/Documents/DABS/Appunti/Computer_Programming_and_Data_Management/Files/numbers.txt'
fin = open(path, encoding="utf-8")

for line in fin:
  print(line)
  
fin.close()
```

The `with` statement allows you to perform operation on a file and closes it automatically after the last line:

```{python opening and print}
with open(path, encoding="utf-8") as fin:
  x = [int(x)**2 for x in fin]
  print(x)
```

> *Opening modes*:
>
> * `r`: read only.
> * `w`: write only. Overwrites existing file.
> * `x`: write only. Halts if file already exists (`FileExistError`).
> * `a`: append to existing file.
> * `r+`: read and write.
> * `b`: add to mode for binary files (i.e. `rb`).
> * `t`: text mode for files (automatically decdes bytes to Unicode).

```{python writing to file 1, error = T}
file_folder_path = '/Users/themagician/Documents/DABS/Appunti/Computer_Programming_and_Data_Management/Files/'
with open(file_folder_path + 'test.txt', mode = 'x', encoding="utf-8") as fin:
  fin.write('Hello World')
```

```{python writing to file 2}
with open(file_folder_path + 'test.txt', mode = 'w', encoding="utf-8") as fin:
  fin.write('This is a test. DO NOT PANIC.\n')
  fin.write("Well, since we're here...\n")
  fin.write("HELLO WORLD!")
```




# Classes and Objects

***Object-Oriented_Programming*** has a powerful underlying logic that allows access to powerful computing patters. It uses programmer-defined types to organise both ***code*** and ***data***: you can define your *own* classes.

> A **class** is a user-defined prototype/type from which objects are created (or instantiated). Classes provide a means of bundling data and functionality together. Each class instance can have attributes attached to it for maintaining and managing its state, stored in the encapsulated data. Class instances can also have methods for modifying its state.

A **function** is not associated to any **space** or ***state**, while a ***method*** operates on the ***state*** of an ***object***. **Class** definition creates a **prototype**, which can be used later by invoking methods, which are functions that **change the state** (for example, the variables in the object name space) of an objects, stored inside the encapsulated data, and are just functions with an implicit first parameter called by convention **self**.


## The Self

Class methods must have an **extra first parameter** in the method definition. If we have a method which takes no arguments, then we have one argument. This first parameter is conventionally named `self` as a reference to the *prototype* object.

When we invoke a method as `myobject.method(arg1, arg2)` this is automatically converted by Python into `MyClass.method(myobject, arg1, arg2)`: you do not need to refer to the **state** of your **prototype**, it is implicit in the *method invocation*.


## Defining a New Type

Let us define a type/data structure to represent a **person**; we are interested in some of its attributes, such as *name, gender and age*. By convention, class names are defined using the first letter in capital.

By convention, class names are defined by ***capitalising the first letter***.

```{python ooj: class definition}
class Person:
  pass

me = Person()
type_print(me)
```

A good design choice is to define functions that **change the state of an object as a *method* ** of the used-defined class.

First, define *static* attributes of a class. These are structured choice of the **class**, which are **inherited** to any other objects of the same type.

```{python ooj: static attributes}
class Person:
  name = None
  gender = None
  age = None
    
  def one_year_older(self):
    self.age += 1

me = Person()
me.name = 'Marco'
me.gender = 'M'
me.age = 16

print(me.age)
```

> Any class method has to be defined ***inside*** the class statement.

```{python ooj: methods}
me.one_year_older()
print(me.age)
```

---




# `NumPy`

`Numpy` is one of the most important packages for ***numerical computing*** in `python`. It is also a *foundational* package, in the sense that many computational scientific package build upon or use its functionalities and data structures, such as the **matrix array** object. Used in combination with `pandas` (it does not actually have *modelling* or *scientific* functionalities), yields powerful and fast system of operating on data: it is *designed* 

> What are its key functionalities?
>
> * `ndarray`: an efficient multidimensional **array** for fast array-oriented arithmetic operations.
> * Functions for **fast operations on entire arrays** (no for loops required).
> * Tools for **reading/writing array data to disk** and **working with memory-mapped files**.
> * Linear algebra, random number generation, Fourier transform.
> * A `C` API.
>
> What can `Numpy` do for ***data analysis***?
>
> * **Fast array-based operation** for data munging and cleaning, subsetting, filtering, transformation.
> * **Common array algorithms** like sorting, unique and set operations.
> * Efficient **descriptive statistics** and **aggregating/summarising** data-
> * **Data alignment** and **relational data manipulations** for merging and joining heterogeneous datasets.
> * Expressing **conditional logic as *array expressions**.
> * **Group-wise data manipulation** (aggregation, transformation and function application).


## `ndarray`: Multidimensional Array Object

`ndarray` is a $N$-dimensional **array object**, built to contain data in a fast and flexible way. Arrays allows you to perform *mathematical operations* on whole blocks of data using *similar syntax* to the equivalent operations between *scalar elements*.

`np` is a standard naming convention for package:

```{python import and array initialize 1}
import numpy as np

data = np.array([[1.5, -0.1, 3], [0, -3, 6.5]])

type_print(data)

```

Another way to initialize an array is to use the `array` function, which accepts **any sequence-like** homogeneous objects:

```{python array initialize 2}
num_sequence = [ [1.5, -0.1, 3], [0, -3, 6.5] ]
data = np.array(num_sequence)

type_print(data)
```

**Nested lists** will be converted into **multi-dimensional** arrays. To flatten an array:

```{python flatten}
data.flatten()
```

```{python array arithmetic 1}
data * 10
```

```{python array arithmetic 2}
data + data**2
```

> In other words, operators work **element-wise** with `ndarray`s.

Data must be **all of the same type**; each array has the following **methods** and **characteristics**:

* `*.shape`: *tuple* indicating the size of each dimension.
* `*.dtype`: describes the **data type** of the array.
* `*.ndim`: checks the **number of dimensions** of the array.

```{python shape and dtype}
print('shape =', data.shape, '\ndtype =', data.dtype, '\nndim =', data.ndim)
```

> These invokations refers to what we might call the **metadata** of an array.

Unless specified, `numpy.array` tries to infer a good data type for the array that is creates. The data type is stored in the `dtype` metadata object; since it is a computing-oriented library, `float64` will be the default value for most use cases.

Specific kind of arrays can be initialized by calling pre-made functions. To create ***multi-dimensional arrays***, pass a **tuple** as the *dimension* argument.

* `np.zeros`

```{python void array}
np.zeros((3, 5))
```

* `np.ones`

```{python homogeneous matrix}
np.ones((3,5))
```

* `np.empty`: creates an array without initializing its values (it might contain *any* value)

```{python empty array}
np.empty((5,5,3))
```

* `np.arrange`: array-valued version of `range`.

```{python arrange}
np.arange(10)
```

* `eye`, `identity`: create a square $N \times N$ identity matrix.

```{python identity matrix}
np.identity(10)
```


### Data Types for `ndarrays`

`dtype` is a special metadata object containing all the information `python` needs to interpret a chunk of memory as a particular type of data. Encoding data in `NumPy` allows to interoperability with other programming languages and systems, because it ensures a common *language* when storing information in memory.

```{python data types}
e1_arr = np.array([1, 2, 3], dtype = np.float64)
e2_arr = np.array([1, 2, 3], dtype = np.int32)

type_print(e1_arr)
type_print(e2_arr)
```

To convert (***cast***) between data types, invoke the `astype` method:

```{python changing data type}
e2_arr = e2_arr.astype(np.float64)
e2_arr.dtype
```

This method allows to convert numbers stored as strings in the correct format:

```{python cast strings into float 1}
numeric_strings = np.array(["1.25", "-9.6", "42"])
numeric_strings
```

```{python cast strings into float 2}
numeric_strings.astype(float)
```


### Arithmetic with `ndarrays`

> The fundamental use case for **arrays** is to express batch operations on data ***without writing any `for` loops***: this process is called **vectorization**.

We have already seen how arithmetic operations between **equally sized arrays** are computed *element-wise*. If the operations involve **scalars**, the scalar argument is propagated to each element of the array:

```{python array arithmetic}
1/(e1_arr*e2_arr)
```


### Indexing and Slicing

Slicing one dimensional arrays  works a lot like lists:

```{python array slicing}
e3_arr = np.arange(1, 21)
e3_arr[5:10] ; e3_arr[10] ; e3_arr[4:]
```

This kind of subsetting can be also used to modify an array:

```{python array subsetting and modification}
e3_arr[19] = 3.14 ; e3_arr[1:5] = 0
e3_arr
```

Notice how the type is automatically cast to maintain homogeneity between each element; moreover, slicing assignment is ***broadcast*** to all subsetted element with the corresponding indices.

> An important first distinction from `lists` is that array slices are **views on the original array**: this means that ***the data is not copied, and any modifications to the view will be reflected in the source array***.

If you wish to *copy* instead of *viewing*, you have to declare it explicitly with the `*.copy()` method.

With **higher dimensional array** ($N$ dimensions) subsetting with a scalar accesses sub-arrays of $N -1$ dimensions: for example, with bi-dimensional arrays, subsetting yields a one-dimension array.

```{python higher dimensions and subsetting 1}
e4_data = np.array([[1.5, -0.1, 3], [0, -3, 6.5]])
e4_data[1]
```

Individual elements can be accessed **recursively**; you can also pass a **comma-separated list of indices** to select individual elements:

```{python higher dimensions and subsetting 2}
e4_data[0,1] ; e4_data[0][0]
```

Indexing and slicing works a little bit differently than usual: **rows** are considered a sort of ***axis-0***, while **columns** a ***axis-1***. Multiple slices can be passed down as arguments.

![](/Users/themagician/Documents/DABS/Appunti/Computer_Programming_and_Data_Management/Notes/Period 2/CP_Notes_RMarkdown/array_indexing.png)

Some examples:

```{python slicing and indexing}
e5_arr = np.array([[1,2,3],[4,5,6],[7,8,9]])
e5_arr ; print(); e5_arr[1:,:2] ; print(); e5_arr[0, 1:]
```

```{python modifying with indexes}
e5_arr[1:,1:] = 0
e5_arr
```

Indexing can also be computed by passing an **integer array** as a subsetting argument. For example:

```{python fancy indexing 1}
arr = np.zeros((8, 4))
for i in range(8):
  arr[i] = i
  
arr
```

```{python fancy indexing 2}
arr[[3,1,4]] ; print() ; arr[[-3,-1,-4]]
```

Passing multiple index arrays selects a one-dimensional array of elements corresponding to each tuple of indices:

```{python fancy indexing multiple arrays}
arr = np.arange(32).reshape((8,4))

arr; print(); arr[[1,5,7,2],[0,3,1,2]]


```

The selected elements corresponds to the indices given by the following tuples:`(1,0),(5,3)`...and so on.


### Boolean Indexing

Like *arithmetic*. **Boolean operations are *vectorised* **: this yields a powerful way to subset by using arrays with the same number of dimensions. Moreover, different types of indexing can be matched together.

```{python boolean indexing 1}
names = np.array(["Bob", "Joe", "Will", "Bob", "Will", "Joe", "Joe"])

data = np.array([[4, 7], [0, 2], [-5, 6], [0, 0], [1, 2], [-12, -4], [3, 4]])

names; print(); data
```

```{python boolean indexing 2}
names == 'Bob'
```

```{python boolean indexing 3}
data[names == 'Bob', 1:]
```

You can use all logical operators except `and` and `or` (use `&` and `|`); `~` is equivalent to `!=`, it negates the condition given and works when a *Boolean array is * **referenced by a variable**.

```{python boolean indexing 4}
cond = names == 'Bob'
data[~ cond]
```

This code works also for **setting data**:

```{python boolean indexing setting data}

data[data < 0] = 0
data
```


### Transposing Arrays and Swapping Axes

The `*.transpose` method and `T` attribute allow transposed views of the underlying data:

```{python transpose}
data.transpose() ; print(); arr.T
```

This is an essential method necessary to perform **matrix computations**, such as the ***inner matrix product*** with `dot()` or the `@` operator.

```{python inner product}
np.dot(arr.T, arr)
```

To swap axes, use the `*.swapaxes` method, which takes a pair of axis numbers and switches the indicated axes to rearrange the data.




## Pseudorandom Number Generation

The `numpy.random` module supplements built-in `random` with function, which generates one value at a time, for efficiently generating whole arrays of sample values from many kinds of probability distributions, allowing efficient and fast ***pseudorandom*** number generation.

```{python random normal sample}
samples = np.random.standard_normal(size = (5,5))

type_print(samples)
```

The generator can be configured by explicit setting. The `seed` argument is what determines the **initial state** of the generator, which *changes * ** each time ** *the  `rng` object is used to generate data *.

```{python setting seed}
rng = np.random.default_rng(seed = 12345)

samples = rng.standard_normal(size = (2,3))

type_print(rng); print(); type_print(samples)
```




## Universal Functions

**ufunc**, or ***universal function***, is a function that performs element-wise operations on data in *ndarrays*. They are effective *vectorised wrappers*  for simple functions that take one or more scalar values and produce one or more scalar results. For example, `numpy.sqrt` and `numpy.exp` are simple element-wise transformations, or ***unary*** functions:

```{python np sqrt}
np.sqrt(arr)
```

```{python np exp}
np.exp(arr)
```

Other functions take *two arrays* as arguments (thus, **binary** ufuncs) and return a single array as a result:

```{python np binary ufuncs}
x = rng.standard_normal(size = (3,3))
y = rng.standard_normal(size = (3,3))

np.maximum(x,y)
```

The `maximum` method computed the ***element-wise*** maximum of the given arrays.

While not common, a ufunc can also return *multiple* arrays; `numpy.modf` is a vectorized version of `math.modf`. which returns the *fractional* and *integral* part of a floating-point array.

```{python np modf}
remainder, whole_part = np.modf(x)

remainder; print() ; whole_part
```

All ufuncs accepts an optional argument, `out`, which allows to assign the result of the computation to a pre-existing variable.

```{python ufuncs out}
out = np.zeros_like(x)

np.exp(x, out = out) == out
```


## Array-Oriented Programming with Arrays

***Vectorization***: expressing data processing tasks as concise array expressions that might otherwise require writing loops.

For example, if we wish to evaluate the function `sqrt((x^2 + y^2))` across a **grid of values**. `numpy.meshgrid` takes *two one-dimensional arrays* and produces two *two dimensional matrices* corresponding to all pairs of `(x, y)` in the two arrays:

```{python meshgrid}
points = np.arange(-5, 5, 0.01)

xs, ys = np.meshgrid(points, points)

xs; print(); ys
```

You can then evaluate the expression by writing the **same code you would use to compute the operation on two points**:

```{python meshgrid ex}
z  = np.sqrt(xs ** 2 + ys ** 2)
z
```


### Expressing Conditional Logic as Array Operations

Suppose that we wanted to take a value from `xarr` wherever the corresponding value in `cond` is `True` and otherwise take the value from `yarr`.

```{python vectorised conditionals 1}
xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
cond = np.array([True, False, True, True, False])
```

List comprehension will be slow and would not work with multidimensional arrays. The function `numpy.where`, instead, allows us to perform this task with a simple function call.

```{python vectorised conditionals 2}
np.where(cond, xarr, yarr)
```

The second and third argument could also be ***scalars***; a typical use case for this function is to produce a new array of values based on another array.

For example, you might want to replace all positive values with 2 and all negative values with -2:

```{python vectorised conditionals 3}
arr = rng.standard_normal((4, 4))

arr; print(); arr > 0
```

```{python vectorised conditionals 4}
np.where(arr > 0, 2, -2)
```


### Mathematical and Statistical Methods

Mathematical and statistical functions are loaded with `NumPy` as ***array methods***: they can either be **reductions**, such as `sum`, `mean`, `std`:

```{python computing stat indicators}
arr = rng.standard_normal((5,5))
arr

np.mean(arr)

arr.sum()

arr.std()
```

These functions can take another argument, `axis`, which tells the software to compute the statistic over the given axis, resulting in an array with one less dimension. 

* `axis = 1`: compute down ***rows***.
* `axis = 0`: compute across ***columns***.

```{python axis argument}
arr.mean(axis = 1)
```

Other methods, such as `cumsum` or `cumproducts` (cumulative *sum* or *product*), ***do not aggregate***, producing an array of the intermediate results.

```{python cumsum}
arr.cumsum()
```

 
### Methods for Boolean Arrays
 
Booleans are coerced into sequences of 1 and 0 (`True`= 1, or any number different than 0; `False` = 0); `sum` yields a quick way of counting the ratio in a ***Boolean array***:

```{python boolean arrays methods 1}
arr = rng.standard_normal(100)
(arr > 0).sum()
(arr <= 0).sum()
```

The parentheses are necessary to invoke a method on the temporary result of the logical expression. Other available methods are `all` and `any`: the first check if ***all values are `True`***, while the second if at least one of the array's value is.


### Sorting


The same `sort` methods *sorts* arrays in place:

```{python sort}
arr = rng.standard_normal((3,3))
arr.sort()
arr
```

The `axis` argument may be provided: in this case, each one-dimensional component of a multi-dimensional array is sorted following the specified direction, either by row or by column.

`np.sort()`, instead, returns a sorted copy of the array.

```{python np.sort}
arr = rng.standard_normal((3,3))
arr_copy = np.sort(arr)
arr; print(); arr_copy
```


### Unique and Other Set Logic

This operations work on ***one-dimensional arrays***:

* `unique`: returns an array containing all the unique values.

```{python unique}
names = np.array(["Bob", "Will", "Joe", "Bob", "Will", "Joe", "Joe"])
np.unique(names)
```

* `numpy.in1d`: tests membership of the values in one array in another, returning a Boolean array.

```{python in1d}
values = np.array([6, 0, 0, 3, 2, 5, 6])
np.in1d(values, [2, 3, 6])
```

* `intersect1d(x, y)`: computes the sorted intersection of two arrays.
* `union1d(x, y)`: computes the sorted union of two arrays.


## File Input and Output With Arrays

`numpy.save` and `numpy.load` are the two workhorse functions for efficiently saving and loading array data on disk. Arrays are saved by default in an uncompressed raw binary format with file extension `.npy`:

```{python numpy load and save}
arr = np.arange(10)
np.save('some_array', arr)
np.load('some_array.npy')
```

`numpy.savez` stores your data in an uncompressed file, while `numpy.savez_compressed` uses compression.


## Linear Algebra

`numpy.linalg` has a standard set of matrix decompositions and things like inverse and determinant:

```{python linalg 1}
from numpy.linalg import inv, qr

X = rng.standard_normal((5, 5))

X
```

```{python linalg 2}
mat = X.T @ X
inv(mat)
```

```{python linalg 3}
mat @ inv(mat)
```

* `diag`:	return the diagonal (or off-diagonal) elements of a square matrix as a 1D array, or convert a 1D array into a square matrix with zeros on the off-diagonal.
* `dot`: matrix multiplication.
* `trace`: compute the sum of the diagonal elements.
* `det`: compute the matrix determinant.
* `eig`: compute the eigenvalues and eigenvectors of a square matrix.
* `inv`: compute the inverse of a square matrix.
* `pinv`: compute the Moore-Penrose pseudo-inverse of a matrix.
* `qr`: compute the QR decomposition.
* `svd`: compute the singular value decomposition (SVD).
* `solve`: solve the linear system $A \times x = b$ for $x$, where $A$ is a square matrix.
* `lstsq`: compute the least-squares solution to $A \times x = b$.

## Example: Random Walks

Pure `python` implementation:

```{python randomwalk vanilla, fig.width = 8, fig.height = 8, fig.align='center'}

import random
import matplotlib.pyplot as plt

plt.close()

position = 0
walk = [position]
nsteps = 1000

for _ in range(nsteps):
    step = 1 if random.randint(0, 1) else -1
    position += step
    walk.append(position)
    
plt.show(plt.plot(walk[:100]))

```

This ***random walk*** can easily be translated in `NumPy`:

* `walk` is the `cumsum` of the **random steps** and could be evaluated as an **array expression**.
* `numpy.random` can be used to draw 1,000 coin flips at once, set these to 1 and –1, and compute the cumulative sum:


```{python randomwalk numpy, fig.align='center'}
nsteps = 1000

rng = np.random.default_rng(seed=12345)  

draws = rng.integers(0, 2, size=nsteps)

steps = np.where(draws == 0, 1, -1)

walk2= steps.cumsum()

plt.close()
plt.show(plt.plot(walk2[:200], color = 'darkorange'))
```

---




# `pandas`

`pandas` is a package that allows Python to use ***tabular data***: one of the most important structure to store data in an easy way, similar to `SQL` (without the need to learn the language) or `R`'s `dataframe`. It is a fundamental data structure for ***data analytics***, yielding powerful tool for *data cleaning, exploration and also machine learning*, building on top of `NumPy`'s `ndarray` (the main difference being that `pandas` is designed to work with **heterogeneous** data, while `NumPy` works best with *homogeneous numerical arrays*).

```{python import pandas}
import pandas as pd
from pandas import Series, DataFrame, RangeIndex
```

Usually, `Series`, `DataFrame` and `RangeIndex` are imported directly into the workspace.

* `Series`, which is a **sequence** (*uni-dimensional array*) with associated labels for each of its elements. It is the most basic structure.
* `DataFrame`: it is a *bi-dimensional array* with rows and columns, which is a table-like representation of a data matrix with labels for columns.

```{python pd import and workspace setup}
PREVIOUS_MAX_ROWS = pd.options.display.max_rows
pd.options.display.max_rows = 20
```

## Data Structures

### `Series`

A `Series` is a one-dimensional array-like object containing a sequence of values and an associated array of data labels called its index. Conceptually it expresses a collection of ***measures***, each one being related to a specific label.T hey must not be confused with *time series*, even though evidently they can be used to store and represent them.

The simplest `Series` is formed by an **array of data**:

```{python ex Series 1}
obj = Series([4, 7, -5, 3])

type_print(obj)
```

Series display an **index** on the left; if you do not assign an index, you get as default a `RangeIndex` of range $[0, N-1]$.

The `*.index` method displays the **index** and its **characteristics**:

```{python index RangeIndex}
obj.index
```

The `*.array` method, instead, prints a `PandasArray` representation of the `Series`: it is basically a *wrapper* around a `NumPy` array, but can also contain special *array extension* types:

```{python array representation}
obj.array
```

You can define a custom index by calling `RangeIndex` ***after the series assignment***. As with `dict`, indexes can be made of all *hashable* types; `tuples` and *type mixing* is allowed, as is **having multiple index with the same value**.

```{python series custom index}
obj = Series([4, 7, -5, 3], index = ['d', 'b', 'a', 'a'])

type_print(obj)
```

You can **access any element of a series** with its named index and `[]`:

```{python retrieving elements}
type_print(obj['d'])
```

Interestingly, if you have *the same object as index for multiple elements*, subsetting yields another series, a sub-series; this is a typical property of ***relational databases***:

```{python same index multiple objects}
type_print(obj['a'])
```

Any time you **subset for multiple elements**, you get another `Series` object as a result.

```{python indexing set of values}
type_print(obj[['b','d']])
```

As with any mutable object in Python, **assigning a new value** is straightforward:

```{python assigning new values to series}
obj['d'] = 3.14

print(obj)
```

If you do a mutiple subsetting, **the order of the subsetting is the same of the values of the resulting object**:

```{python multiple subsetting}
type_print(obj[['a', 'b', 'd']])
```

> Be sure to pass the **proper object** as an index. Indexes can be also added later.

We can also add more information to improve *readability*; for example, you can add **labels** and **named indexes** (attribute `name`):

```{python labels and named indexes}
studentsIDs = [872429, 872209, 873229, 000000]
indexName = "Student ID"

students = pd.Series(['Elena', 'Mattia', 'Veronica', 'David'], index=studentsIDs)
students.name = "Names of the students from Volpe Group"
students.index.name = indexName
students
```

Using `NumPy` functions or `NumPy`-like operations, such as *filtering with a boolean array*, *scalar multiplication*, or *applying mathematical functions*, will preserve the index-value link:

```{python numpy & series boolean}
type_print(obj[obj.values > 0])
```

```{python numpy methods on series}
np.exp(obj)*np.log(2)
```

Another way to think about a Series is as a ***fixed-length, ordered dictionary***, as it is a **mapping** of **index values** to **data values**.

```{python boolean operators series}
'a' in obj
```

This allows you to perform complex assignments:

```{python complex assignment series}
obj[obj > 5] = 'NULL'

obj
```

You can use a `dict` object to initialize a `Series`; missing values will be shown as `NaN`s. The inverse operation can be performed with the `*.to_dict` method.

```{python initialize series with dict}
sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
states = ['California', 'Ohio', 'Oregon', 'Texas', 'Utah']
obj2 = pd.Series(sdata, index=states)
type_print(obj2)
```

```{python convert series to dict}
type_print(obj2.to_dict())
```

To find `null` values, you can invoke the method `*.isnull` or `*.isna`:

```{python find null values}
obj2.isna()
```

To subset for all *not null* values, invoke the method `*.notnull` or `*.notna`:

```{python na methods}
obj3 = obj2[obj2.notna()]
obj3
```

These two are specific **`Series` methods** which can be invoked on any `Series` object; they can be also called as function with `pd.isna(obj)`.

A sum of two different series will be performed only on elements which are present in both objects.

```{python Series slicing and indexing}
obj.values[:] = [4,3,2,1]
print(obj)
obj[:] = [1,2,3,4]
print(obj)
```

### `DataFrame`

`DataFrame` are a really powerful data-structure: it represents a **rectangular table of data** and contains an *ordered collection* of columns, each of which can be composed of a different data type^[It can be thought as a *series of dictionaries* all sharing the same index.]. Tabular data is hugely popular and comes in many different formats, such as `.xlsx` or `.csv`.

We can initialise by *converting* a *dictionary of lists* (or, usually, a dictionary of *arrays*):

```{python DataFrame init 1}
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002, 2003],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
frame = pd.DataFrame(data)
frame
```

Indexes are created automatically; the columns are placed according to the order of the keys in `data`. As an alternative, a *nested lists* can be used and labels can be added later:

```{python DataFrame init 2}
datanp = np.array([['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        [2000, 2001, 2002, 2001, 2002, 2003],
        [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]])
frame = pd.DataFrame(datanp.T,columns=['state','year','pop'])
frame
```

If initialized with a **nested dictionary of dictionaries** `pandas` will interpret the outer dictionary *keys* as the *columns*, and the *inner keys* as the *row indices*.

`DataFrame`s have a lot of useful methods, such as `*.head()`, which will only *print on screen the first $n$ rows of a `DataFrame`*:

```{python head method}
frame.head(3)
```

Similarly, `tail` returns the last $n$ rows.

We can also pass a **custom index** as an *extra argument*:

```{python dataframe custom index}
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002, 2003],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
frame2 = pd.DataFrame(data, columns=['year', 'state', 'pop', 'debt'],
                      index=['one', 'two', 'three', 'four',
                             'five', 'six'])
frame2
```

Notice that: the columns are arranged as specified in the given *initialization sequence*; if the index dimensions do not match the number of columns, **the missing column values are filled with `NaN`s**. If the number of **rows** does not match, instead, the program halts and a `ValueError` is generated.

> Having indexes is fundamental when operating with databases, because they allow operation such as **join** which are performed much faster by matching **indexes** instead of **values**.

A `DataFrame` column can be retrieved (as a `Series` object) either by `dict`-like notation or by attribute. It will inherit the parent `DataFrame` index.

```{python column retrieval}
type_print(frame2['pop'])
```

Another useful syntax to subset is to use the so-called ***attribute access**:

```{python column retrieval with attrivute access}
frame2.year
```

> **Warning**: works only when the column name is a ***valid `python` variable name*** and does not conflict with any of the method names in `DataFrame`.

**Rows** can also be retrieved by position or name with the special `loc` or `iloc` attributes:

```{python row retrieval}
frame2.loc['two']
```

*Columns can be modified by assignment*, either with a **scalar** (which will be *replicated* to match the required length) or with an **array-like object** (which *must* be of the same length or it will prompt a `ValueError`).

```{python column modification}
frame2['debt'] = 16.5
frame2
```

We can perform multiple complex assignments with these tools:

```{python complex assignments 1}
val = pd.Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])
val
```

```{python complex assignments 2}
frame2['debt'] = val
frame2
```

Notice how most of the `DataFrame` methods share their syntax with `list` objects:

```{python list methods with dataframes}
type_print(frame2.values)
```

In this case, the `*.values` method extracs a bi-dimensional array object.

Moreover the `del` keyword will delete columns ike with a *dictionary*.

```{python del keyword 1}
frame2["eastern"] = frame2["state"] == "Ohio"
frame2
```

```{python del keyword 2}
del frame2["eastern"]
frame2.columns
```

> **Warking**: the column returned from indexing a `DataFrame` is **a view on the underlying data, not a copy**. Thus, any in-place modifications to the `Series` will be reflected in the `DataFrame.` The column can be explicitly copied with the Series’s copy method.

You can transpose the `DataFrame` (swap rows and columns) with similar syntax to a NumPy array:

```{python dataframe transpose}
frame2.T
```

> Transposing **discards the column data types if the columns *do not all have the same data type* **, so transposing and then transposing back may lose the previous type information. 

If a `DataFrame`’s index and columns have their name attributes set, these will also be displayed.

```{python naming dimensions}
populations = {"Ohio": {2000: 1.5, 2001: 1.7, 2002: 3.6}, "Nevada": {2001: 2.4, 2002: 2.9}}
frame3 = pd.DataFrame(populations)

frame3.index.name = "year"

frame3.columns.name = "state"

frame3
```


### `Index`

`Index` objects are responsible for holding the **axis labels** (for example, `DataFrame`'s column names) and other **metadata** (the axis name or names): any array or sequence used for labelling is converted and stored to this specific type.

```{python index obj}
obj = pd.Series(np.arange(3), index=["a", "b", "c"])
type_print((obj.index))
```

Index are subsettable and sliceable, like other **sequences**; however, they are ***unmutable***:

```{python index obj are unmutable, error = T}
obj.index[1] = 'd'
```

This feature makes it safer to **share indexes between multiple objects**. In addition to being array-like, an Index also behaves like a **fixed-size set**; however, they can contain *duplicate labels* and selecting such a label will return *all associated items*.

> Each Index has a number of methods and properties for **set logic**, which answer other common questions about the data it contains, such as `intersection`, `union`, `append` (since *indexes are unmutable*, this kind of methods create a **new index**).


## Essential Functionality

> Fundamental mechanics of *interacting* with the data contained in a `Series` or `DataFrame`.


### Reindexing

An important method on pandas objects is `reindex`, which means **to create a new object with the data conformed to a new index**.

```{python reindexing 1}
obj = pd.Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])
obj
```

```{python reindexing 2}
obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])
obj2
```

We can ***reindex an object by the indexes of another one***; this way we will have ***matching indexes*** in both.

```{python reindexing 3}
obj3 = pd.Series([24.5, 72.2, -25.3, 23.6], index=['a', 'd', 'c', 'b'])
obj3
```

```{python reindexing 4}
obj2.reindex(obj3.index)
```

For ordered data like **time series**, it may be desirable to do some *interpolation*  of or *filling* values when reindexing. Invoking specific methods allows us to do this, using a method such as `ffill`, which **forward-fills the values**.

```{python ffil interpolation 1}
obj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])

obj3.reindex(range(6))
```

```{python ffil interpolation 2}
obj4 = obj3.reindex(range(6), method='ffill') 
obj4
```

This allows you to **fill gaps in the data**, which is very important when you are working with time series.

With `DataFrame`, `reindex` can alter either the (row) index, columns, or both. When passed only a *sequence*, it **reindexes the rows in the result**:

```{python altering with reindex 1}
frame = pd.DataFrame(np.arange(9).reshape((3, 3)),
                     index=['a', 'c', 'd'],
                     columns=['Ohio', 'Texas', 'California'])
frame
```

```{python altering with reindex 2}
frame2 = frame.reindex(['a', 'b', 'c', 'd'])
frame2
```

```{python inserting values with reindex}
frame2 = frame.reindex(['a', 'b', 'c', 'd'],method='ffill')
frame2
```

The columns can be reindexed with the `columns` keyword:

```{python altering with reindex 3}
states = ['Texas', 'Utah', 'California']
frame = frame.reindex(columns=states)
frame
```

Because `Utah` was not in states, the data for that column is dropped from the result.

`*.loc` can be invoked to perform reindexing:

```{python reindexing with loc}
frame2.loc[["a", "d", "c"], ["California", "Texas"]]
```

It works only if **all of the new index labels already exist in the `DataFrame`** (whereas reindex will insert missing data for new labels).

To **reindex a particular axis** pass the new axis labels as a positional argument and then specify the axis to reindex with the `axis` keyword:

```{python axis keyword}
frame.reindex(states, axis="columns")
```


### Droppoing Entries From an Axis

Aside from operating with `reindex` and `loc`, which require having setting indexes and using **set logic**, entries droppoing might be coded with the `drop` method:

```{python series drop 1}
obj = pd.Series(np.arange(5.), index=["a", "b", "c", "d", "e"])
obj
```

```{python series drop 2}
obj.drop(["d", "c"])
```

With `DataFrame`s, items can be dropped from any axis:

```{python dataframe drop 1}
data = pd.DataFrame(np.arange(16).reshape((4, 4)), index=["Ohio", "Colorado", "Utah", "New York"], columns=["one", "two", "three", "four"])
data
```

Calling `drop` with a ***sequence of labels*** will **drop values from the row labels (axis 0)**:

```{python dataframe drop 2}
data.drop(index=["Colorado", "Ohio"])
```

To **drop labels from the columns**, instead use the `columns` keyword:

```{python dataframe drop 3}
data.drop(columns=["two"])
```


### Indexing, Selection, and Filtering

`Series` indexing `(obj[...])` works *analogously to `NumPy` `array` indexing*, except **you can use the `Series`’s index values** instead of only integers. While you can select data by label this way, the **preferred way** to select index values is with the special `loc` operator:

```{python indexing with loc}
obj = pd.Series(np.arange(4.), index=["a", "b", "c", "d"])
obj.loc[["b", "a", "d"]]
```

This is because `int` object are treated different when using `[]` indexes, while the `loc` method treat all arguments as **labels**; thus it is always consistent.

Assigning values using these methods modifies the corresponding section of the `Series`:

```{python indexing and assigning}
obj.loc["b":"d"] = 5
obj
```

Indexing into a `DataFrame` retrieves one or more columns either with a *single value* or *sequence*:

```{python indexing dataframe 1}
data = pd.DataFrame(np.arange(16).reshape((4, 4)), index=["Ohio", "Colorado", "Utah", "New York"], columns=["one", "two", "three", "four"])

data ; print() ; data[:2] ; print(); data[['one', 'three']]
```

```{python indexing and assigning dataframe}
data[data < 5] = 0
data
```

Since `DataFrame` is two-dimensional, you can **select a subset of the rows and columns** with `NumPy`-like notation using either axis labels (`loc`) or integers (`iloc`). You can combine *row* and *column* indexing by putting a comma between the selections.

```{python indexing dataframe loc}
type_print(data.loc['Colorado', ['two', 'three']])
```

Boolean arrays can be used with `loc` but not `iloc`.


### Arithmetic and Data Alignment

An important `pandas` feature for some applications is that the software will try to perform any given instruction even if **indexes do not match**, by creating *unions of index pairs*.

```{python different indexes operations series, error = T}
s1 = pd.Series([7.3, 5.4, 6, 3.9], index = ['a', 'c', 'd', 'e' ])
s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=["a", "c", "e", "f", "g"])
s1+s2
```

> The **internal data alignment** introduces missing values in the label locations that don’t overlap. Missing values will then propagate in further arithmetic computations.

In the case of `DataFrame`, alignment is performed on both rows and columns:

```{python different indexes operations dataframe 1}
df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=list("bcd"), index=["Ohio", "Texas", "Colorado"])
df2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list("bde"), index=["Utah", "Ohio", "Texas", "Oregon"])

df1 + df2
```

If you add `DataFrame` objects **with no column or row labels in common**, the result will contain all nulls:

```{python different indexes operations dataframe 2}
df1 = pd.DataFrame({"A": [1, 2]})
df2 = pd.DataFrame({"B": [3, 4]})

df1 + df2
```

Using the add method on `df1`, I pass `df2` and an argument to `fill_value`, which substitutes the passed value for any missing values in the operation:

```{python add method fill_value}
df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=list("bcd"), index=["Ohio", "Texas", "Colorado"])
df2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list("bde"), index=["Utah", "Ohio", "Texas", "Oregon"])

df1.add(df2, fill_value = 0)
```

**Arithmetic method are**:

* `add`
* `sub`
* `div`
* `floordiv`
* `mult`
* `pow`

#### Operations between `DataFrame` and `Series`

```{python dataframe and series op 1}
frame = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list("bde"), index=["Utah", "Ohio", "Texas", "Oregon"])
series = frame.iloc[0]

frame - series
```

The subtraction is performed ***once for each row***. This is referred to as **broadcasting**. If an index value is not found in either the `DataFrame`’s columns or the `Series`’s index, the objects will be re-indexed to form the union:

```{python dataframe and series op 2}
series2 = pd.Series(np.arange(3), index=["b", "e", "f"])

frame - series2
```

The `axis` optional argument allow specification of where the matching should be performed.


## Function Application and Mapping

`NumPy` `ufuncs` (*element-wise array methods*) also work with `pandas` objects:

```{python functions and mappings 1}
frame = pd.DataFrame(np.random.standard_normal((4, 3)), columns=list("bde"), index=["Utah", "Ohio", "Texas", "Oregon"])

frame; print(); np.abs(frame)
```

Another frequent operation is applying a function on one-dimensional arrays to each column or row. `DataFrame`’s apply method does exactly this:

```{python apply method}
def f1(x):
  return x.max() - x.min()

frame.apply(f1)
```

If you pass `axis="columns"` to `apply`, the function will be invoked once per row instead. A helpful way to think about this is as *apply across the columns*:

```{python apply + axis}
frame.apply(f1, axis= 'columns')
```

The function passed to `apply` *need not return a scalar value*; it can also return a `Series` with multiple values:

```{python apply return multiple values}
def f2(x):
  return pd.Series([x.min(), x.max()], index=["min", "max"])

frame.apply(f2)
```

Element-wise `Python` functions can be used, too. You can do this with `applymap`:

```{python applymap}
def my_format(x):
  return f"{x:.2f}"

frame.applymap(my_format)
```


## Sorting and Ranking

***Sorting a dataset by some criterion*** is another important built-in operation. To sort lexicographically by row or column label, use the `sort_index` method, which returns a new, sorted object:

```{python sort_index}
obj = pd.Series(np.arange(4), index=["d", "a", "b", "c"])
obj.sort_index()
```


```{python sort_index descending}
obj.sort_index(ascending = False)
```


```{python sort_values}
obj = pd.Series([4, 7, -3, 2])
obj.sort_values()
```